# Database Configuration
# Supported types: milvus, sqlite
# - milvus: Vector database for production use (requires Docker)
# - sqlite: Lightweight database for development or standalone use
DATABASE_TYPE=sqlite

# Milvus Configuration (only used when DATABASE_TYPE=milvus)
MILVUS_HOST=localhost
MILVUS_PORT=19530
DATABASE_NAME=xivmind
# Batch size for Milvus query operations (prevent exceeding size limit)
MILVUS_QUERY_BATCH_SIZE=3000

# SQLite Configuration (only used when DATABASE_TYPE=sqlite)
# Path to the SQLite database file, will be created if not exists
SQLITE_DB_PATH=./data/xivmind.db

# Download Configuration
# Directory where downloaded PDF files will be stored
DOWNLOAD_DIR=./downloads

# arXiv API Configuration
# Maximum number of retries when arXiv API returns 503 error
ARXIV_MAX_RETRIES=3
# Base delay (seconds) for exponential backoff retry
ARXIV_RETRY_BASE_DELAY=1.0
# Number of papers to fetch per API request (max 500)
ARXIV_BATCH_SIZE=50

# Embedding Configuration
# OpenAI API key for embedding (leave empty to use local model)
OPENAI_API_KEY=
# OpenAI embedding model name
OPENAI_EMBEDDING_MODEL=text-embedding-ada-002

# Local Embedding Model Configuration
# Set to true to force using local model instead of OpenAI
USE_LOCAL_EMBEDDING=true
# Local model name (from sentence-transformers)
# Options: all-MiniLM-L6-v2, paraphrase-multilingual-MiniLM-L12-v2,
#          BAAI/bge-m3, BAAI/bge-large-zh, BAAI/bge-large-en,
#          BAAI/bge-base-en, BAAI/bge-base-zh
LOCAL_EMBEDDING_MODEL=BAAI/bge-m3

# Device for embedding computation
# Options: auto, cuda, mps, cpu
# - auto: Automatically detect (cuda > mps > cpu)
# - cuda: Use NVIDIA GPU
# - mps: Use Apple Silicon GPU
# - cpu: Use CPU only
EMBEDDING_DEVICE=auto
# Batch size for embedding computation (larger = faster but more memory)
EMBEDDING_BATCH_SIZE=32

# Hugging Face Endpoint
# Mirror for downloading models (useful for users in China)
# Options: https://huggingface.co (official), https://hf-mirror.com (China mirror)
HF_ENDPOINT=https://hf-mirror.com

# LLM Configuration for AI Assistant
# Provider: openai, anthropic, glm, ollama
LLM_PROVIDER=openai
# Model name (gpt-4o-mini, gpt-4o, claude-3-haiku-20240307, glm-4-plus, llama3, etc.)
LLM_MODEL=gpt-4o-mini
# Temperature for response generation (0.0 - 1.0)
LLM_TEMPERATURE=0.7
# Maximum tokens in response
LLM_MAX_TOKENS=2048

# GLM (Zhipu AI) Configuration
# Get API key from: https://open.bigmodel.cn
GLM_API_KEY=
GLM_BASE_URL=https://open.bigmodel.cn/api/paas/v4

# Ollama (Local LLM) Configuration
# Install Ollama from: https://ollama.ai
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3
